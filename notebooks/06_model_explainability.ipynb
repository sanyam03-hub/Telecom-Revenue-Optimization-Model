{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74eb2d1d",
   "metadata": {},
   "source": [
    "# Model Explainability with SHAP\n",
    "\n",
    "This notebook demonstrates how to use SHAP (SHapley Additive exPlanations) to explain the predictions of our telecom revenue optimization models. SHAP provides consistent, locally accurate feature importance values that sum up to the difference between the expected model output and the current model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For SHAP\n",
    "import shap\n",
    "\n",
    "# For models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, mean_squared_error\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e075d92",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Let's load our processed telecom customer data for model explainability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3942b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master dataset\n",
    "df = pd.read_csv('../data/raw/master_dataset.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e44f8",
   "metadata": {},
   "source": [
    "## 2. Churn Prediction Model Explainability\n",
    "\n",
    "Let's first explain our churn prediction model using SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for churn prediction\n",
    "churn_features = [\n",
    "    'age', 'annual_income', 'tenure_months', 'arpu', 'satisfaction_score',\n",
    "    'monthly_data_gb', 'monthly_minutes', 'ott_usage_hours',\n",
    "    'monthly_web_sessions', 'monthly_app_sessions', 'self_service_transactions',\n",
    "    'num_complaints_12m', 'support_tickets_12m', 'late_payments_12m',\n",
    "    'campaigns_exposed', 'total_clicks', 'total_conversions'\n",
    "]\n",
    "\n",
    "# Create additional features\n",
    "df['risk_score'] = (\n",
    "    (10 - df['satisfaction_score']) * 0.4 +\n",
    "    df['num_complaints_12m'] * 0.3 +\n",
    "    df['late_payments_12m'] * 0.3\n",
    ")\n",
    "\n",
    "df['usage_efficiency'] = (\n",
    "    df['monthly_data_gb'] / (df['data_allowance_gb'] + 1e-6) +\n",
    "    df['monthly_minutes'] / (df['minutes_allowance'] + 1e-6)\n",
    ") / 2\n",
    "\n",
    "df['value_score'] = pd.qcut(df['arpu'], q=5, labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "# Add enhanced features\n",
    "churn_features.extend(['risk_score', 'usage_efficiency', 'value_score'])\n",
    "\n",
    "X_churn = df[churn_features]\n",
    "y_churn = df['churned']\n",
    "\n",
    "print(f\"Churn prediction features: {len(churn_features)}\")\n",
    "print(f\"Feature columns: {churn_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_churn.shape}\")\n",
    "print(f\"Test set: {X_test_churn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train churn prediction model (LightGBM)\n",
    "print(\"Training LightGBM churn prediction model...\")\n",
    "\n",
    "churn_model = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "churn_model.fit(X_train_churn, y_train_churn)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_churn = churn_model.predict(X_test_churn)\n",
    "y_prob_churn = churn_model.predict_proba(X_test_churn)[:, 1]\n",
    "auc_churn = roc_auc_score(y_test_churn, y_prob_churn)\n",
    "\n",
    "print(f\"Churn model AUC: {auc_churn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f067a",
   "metadata": {},
   "source": [
    "## 3. SHAP Analysis for Churn Model\n",
    "\n",
    "Now let's use SHAP to explain the churn prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98063bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for churn model\n",
    "print(\"Creating SHAP explainer for churn model...\")\n",
    "\n",
    "# Use TreeExplainer for tree-based models\n",
    "churn_explainer = shap.TreeExplainer(churn_model)\n",
    "\n",
    "# Calculate SHAP values for a sample of test data (for performance)\n",
    "sample_indices = np.random.choice(X_test_churn.shape[0], size=1000, replace=False)\n",
    "X_sample = X_test_churn.iloc[sample_indices]\n",
    "\n",
    "print(f\"Calculating SHAP values for {X_sample.shape[0]} samples...\")\n",
    "churn_shap_values = churn_explainer.shap_values(X_sample)\n",
    "\n",
    "print(\"SHAP analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot of feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(churn_shap_values[1], X_sample, show=False)\n",
    "plt.title('SHAP Feature Importance - Churn Prediction Model', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47552280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of mean absolute SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(churn_shap_values[1], X_sample, plot_type=\"bar\", show=False)\n",
    "plt.title('Mean Absolute SHAP Values - Churn Prediction Model', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of top features\n",
    "print(\"=== TOP FEATURES INFLUENCING CHURN PREDICTIONS ===\")\n",
    "\n",
    "# Calculate mean absolute SHAP values\n",
    "feature_names = X_sample.columns\n",
    "mean_abs_shap = np.abs(churn_shap_values[1]).mean(0)\n",
    "\n",
    "# Create DataFrame and sort\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "for i, row in shap_df.head(10).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['mean_abs_shap']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e631b26",
   "metadata": {},
   "source": [
    "## 4. Individual Prediction Explanations\n",
    "\n",
    "Let's examine individual customer predictions to understand how features contribute to specific outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ae665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few high-risk and low-risk customers for detailed analysis\n",
    "probabilities = churn_model.predict_proba(X_sample)[:, 1]\n",
    "\n",
    "# High-risk customer (top 5%)\n",
    "high_risk_idx = np.argsort(probabilities)[-1]\n",
    "high_risk_customer = X_sample.iloc[high_risk_idx:high_risk_idx+1]\n",
    "\n",
    "# Low-risk customer (bottom 5%)\n",
    "low_risk_idx = np.argsort(probabilities)[0]\n",
    "low_risk_customer = X_sample.iloc[low_risk_idx:low_risk_idx+1]\n",
    "\n",
    "print(f\"High-risk customer churn probability: {probabilities[high_risk_idx]:.3f}\")\n",
    "print(f\"Low-risk customer churn probability: {probabilities[low_risk_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51211076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain high-risk customer prediction\n",
    "print(\"=== HIGH-RISK CUSTOMER EXPLANATION ===\")\n",
    "shap.initjs()\n",
    "high_risk_shap = churn_explainer.shap_values(high_risk_customer)\n",
    "shap.force_plot(churn_explainer.expected_value[1], high_risk_shap[1], high_risk_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain low-risk customer prediction\n",
    "print(\"=== LOW-RISK CUSTOMER EXPLANATION ===\")\n",
    "low_risk_shap = churn_explainer.shap_values(low_risk_customer)\n",
    "shap.force_plot(churn_explainer.expected_value[1], low_risk_shap[1], low_risk_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684d895",
   "metadata": {},
   "source": [
    "## 5. ARPU Prediction Model Explainability\n",
    "\n",
    "Let's now explain our ARPU prediction model using SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for ARPU prediction\n",
    "arpu_features = [\n",
    "    'age', 'annual_income', 'tenure_months', 'satisfaction_score',\n",
    "    'monthly_data_gb', 'monthly_minutes', 'ott_usage_hours',\n",
    "    'monthly_web_sessions', 'monthly_app_sessions',\n",
    "    'num_complaints_12m', 'support_tickets_12m', 'late_payments_12m'\n",
    "]\n",
    "\n",
    "X_arpu = df[arpu_features]\n",
    "y_arpu = df['arpu']\n",
    "\n",
    "print(f\"ARPU prediction features: {len(arpu_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_arpu, X_test_arpu, y_train_arpu, y_test_arpu = train_test_split(\n",
    "    X_arpu, y_arpu, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_arpu.shape}\")\n",
    "print(f\"Test set: {X_test_arpu.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8640d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ARPU prediction model (Random Forest)\n",
    "print(\"Training Random Forest ARPU prediction model...\")\n",
    "\n",
    "arpu_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "arpu_model.fit(X_train_arpu, y_train_arpu)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_arpu = arpu_model.predict(X_test_arpu)\n",
    "rmse_arpu = np.sqrt(mean_squared_error(y_test_arpu, y_pred_arpu))\n",
    "\n",
    "print(f\"ARPU model RMSE: ${rmse_arpu:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20703b",
   "metadata": {},
   "source": [
    "## 6. SHAP Analysis for ARPU Model\n",
    "\n",
    "Now let's use SHAP to explain the ARPU prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for ARPU model\n",
    "print(\"Creating SHAP explainer for ARPU model...\")\n",
    "\n",
    "# Use TreeExplainer for tree-based models\n",
    "arpu_explainer = shap.TreeExplainer(arpu_model)\n",
    "\n",
    "# Calculate SHAP values for a sample of test data\n",
    "sample_indices_arpu = np.random.choice(X_test_arpu.shape[0], size=1000, replace=False)\n",
    "X_sample_arpu = X_test_arpu.iloc[sample_indices_arpu]\n",
    "\n",
    "print(f\"Calculating SHAP values for {X_sample_arpu.shape[0]} samples...\")\n",
    "arpu_shap_values = arpu_explainer.shap_values(X_sample_arpu)\n",
    "\n",
    "print(\"SHAP analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot of feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(arpu_shap_values, X_sample_arpu, show=False)\n",
    "plt.title('SHAP Feature Importance - ARPU Prediction Model', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of mean absolute SHAP values\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(arpu_shap_values, X_sample_arpu, plot_type=\"bar\", show=False)\n",
    "plt.title('Mean Absolute SHAP Values - ARPU Prediction Model', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de544fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of top features\n",
    "print(\"=== TOP FEATURES INFLUENCING ARPU PREDICTIONS ===\")\n",
    "\n",
    "# Calculate mean absolute SHAP values\n",
    "feature_names_arpu = X_sample_arpu.columns\n",
    "mean_abs_shap_arpu = np.abs(arpu_shap_values).mean(0)\n",
    "\n",
    "# Create DataFrame and sort\n",
    "shap_df_arpu = pd.DataFrame({\n",
    "    'feature': feature_names_arpu,\n",
    "    'mean_abs_shap': mean_abs_shap_arpu\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "for i, row in shap_df_arpu.head(10).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['mean_abs_shap']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38d3e0",
   "metadata": {},
   "source": [
    "## 7. Uplift Model Explainability\n",
    "\n",
    "Let's explain our uplift modeling for cross-sell/up-sell opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for uplift modeling\n",
    "uplift_features = ['age', 'annual_income', 'tenure_months', 'satisfaction_score',\n",
    "                  'monthly_data_gb', 'arpu', 'risk_score']\n",
    "\n",
    "# Create treatment variable (campaign responsive)\n",
    "df['treatment'] = (df['total_conversions'] > 0).astype(int)\n",
    "df['outcome'] = (df['total_conversions'] > 0).astype(int)  # Binary outcome\n",
    "\n",
    "X_uplift = df[uplift_features]\n",
    "treatment = df['treatment']\n",
    "outcome = df['outcome']\n",
    "\n",
    "print(f\"Uplift modeling features: {len(uplift_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_uplift, X_test_uplift, treatment_train, treatment_test, outcome_train, outcome_test = train_test_split(\n",
    "    X_uplift, treatment, outcome, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_uplift.shape}\")\n",
    "print(f\"Test set: {X_test_uplift.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train uplift models (two-model approach)\n",
    "print(\"Training uplift models...\")\n",
    "\n",
    "# Split into treatment and control groups\n",
    "treatment_mask = treatment_train == 1\n",
    "control_mask = treatment_train == 0\n",
    "\n",
    "X_treatment = X_train_uplift[treatment_mask]\n",
    "y_treatment = outcome_train[treatment_mask]\n",
    "\n",
    "X_control = X_train_uplift[control_mask]\n",
    "y_control = outcome_train[control_mask]\n",
    "\n",
    "# Train separate models\n",
    "treatment_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "control_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "treatment_model.fit(X_treatment, y_treatment)\n",
    "control_model.fit(X_control, y_control)\n",
    "\n",
    "print(\"Uplift models training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e4c7f0",
   "metadata": {},
   "source": [
    "## 8. SHAP Analysis for Uplift Models\n",
    "\n",
    "Let's use SHAP to explain both treatment and control models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainers for uplift models\n",
    "print(\"Creating SHAP explainers for uplift models...\")\n",
    "\n",
    "# Treatment model explainer\n",
    "treatment_explainer = shap.TreeExplainer(treatment_model)\n",
    "\n",
    "# Control model explainer\n",
    "control_explainer = shap.TreeExplainer(control_model)\n",
    "\n",
    "# Sample data for SHAP analysis\n",
    "sample_indices_uplift = np.random.choice(X_test_uplift.shape[0], size=500, replace=False)\n",
    "X_sample_uplift = X_test_uplift.iloc[sample_indices_uplift]\n",
    "\n",
    "print(f\"Calculating SHAP values for {X_sample_uplift.shape[0]} samples...\")\n",
    "\n",
    "# Calculate SHAP values\n",
    "treatment_shap_values = treatment_explainer.shap_values(X_sample_uplift)\n",
    "control_shap_values = control_explainer.shap_values(X_sample_uplift)\n",
    "\n",
    "print(\"SHAP analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot for treatment model\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(treatment_shap_values[1], X_sample_uplift, show=False)\n",
    "plt.title('SHAP Feature Importance - Treatment Model (Uplift)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot for control model\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(control_shap_values[1], X_sample_uplift, show=False)\n",
    "plt.title('SHAP Feature Importance - Control Model (Uplift)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682dc42",
   "metadata": {},
   "source": [
    "## 9. Business Insights from SHAP Analysis\n",
    "\n",
    "Let's extract business insights from our SHAP explainability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BUSINESS INSIGHTS FROM SHAP ANALYSIS ===\")\n",
    "\n",
    "print(\"\\n1. CHURN PREDICTION INSIGHTS:\")\n",
    "print(\"   Key drivers of churn include:\")\n",
    "for i, row in shap_df.head(5).iterrows():\n",
    "    print(f\"   - {row['feature']} (importance: {row['mean_abs_shap']:.4f})\")\n",
    "\n",
    "print(\"\\n2. ARPU PREDICTION INSIGHTS:\")\n",
    "print(\"   Key drivers of revenue include:\")\n",
    "for i, row in shap_df_arpu.head(5).iterrows():\n",
    "    print(f\"   - {row['feature']} (importance: {row['mean_abs_shap']:.4f})\")\n",
    "\n",
    "print(\"\\n3. ACTIONABLE RECOMMENDATIONS:\")\n",
    "print(\"   For Churn Prevention:\")\n",
    "print(\"   - Focus on customers with high risk scores\")\n",
    "print(\"   - Proactively address satisfaction issues\")\n",
    "print(\"   - Monitor payment behavior closely\")\n",
    "\n",
    "print(\"\\n   For Revenue Optimization:\")\n",
    "print(\"   - Target high-value customers for premium services\")\n",
    "print(\"   - Encourage efficient usage patterns\")\n",
    "print(\"   - Leverage digital engagement channels\")\n",
    "\n",
    "print(\"\\n   For Campaign Targeting:\")\n",
    "print(\"   - Use uplift models to identify persuadable customers\")\n",
    "print(\"   - Personalize offers based on customer profiles\")\n",
    "print(\"   - Optimize marketing spend on high-uplift segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a755125e",
   "metadata": {},
   "source": [
    "## 10. Model Governance and Compliance\n",
    "\n",
    "SHAP explanations help ensure our models are transparent and compliant with regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036eff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL GOVERNANCE AND COMPLIANCE ===\")\n",
    "\n",
    "print(\"1. REGULATORY COMPLIANCE:\")\n",
    "print(\"   - GDPR: Transparent decision-making for customer data processing\")\n",
    "print(\"   - CCPA: Right to explanation for automated decisions\")\n",
    "print(\"   - Financial Regulations: Model interpretability for risk assessment\")\n",
    "\n",
    "print(\"\\n2. ETHICAL AI PRACTICES:\")\n",
    "print(\"   - Fairness: Identifying potential bias in feature importance\")\n",
    "print(\"   - Accountability: Clear audit trail of model decisions\")\n",
    "print(\"   - Transparency: Stakeholder understanding of AI systems\")\n",
    "\n",
    "print(\"\\n3. BUSINESS VALUE:\")\n",
    "print(\"   - Trust: Increased confidence in AI-driven decisions\")\n",
    "print(\"   - Optimization: Data-driven insights for business strategy\")\n",
    "print(\"   - Innovation: Safe exploration of new opportunities\")\n",
    "\n",
    "print(\"\\nModel explainability with SHAP successfully implemented!\")\n",
    "print(\"This framework enables transparent, accountable, and effective AI deployment.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
